year_start = input("Which year to start searching from? : ")
year_end = input("Which year to stop searching at? : ")
unique_url = input("Search url after adding filters : ")
page_navigation_url = input("url after pressing 'Next' button : ")
keywords = [str(x) for x in input("keywords (write with comma and space) : ").split(', ')]
compulsory_keywords = [str(x) for x in input("compulsory keywords (write with comma and space) : ").split(', ')]


import urllib.request
from lxml import etree
import pandas as pd
import numpy as np
year_start_int = int(year_start)
year_end_int = int(year_end)
url_year_index = page_navigation_url.index("YR=")
url_p_index = page_navigation_url.index("P=")
years = np.linspace(year_start_int,year_end_int,year_end_int-year_start_int+1).astype(int)
list_of_adverb = keywords
compulsory = compulsory_keywords
for y in range(len(years)):
    full_links_df = pd.DataFrame([])
    url_main = unique_url + str(years[y])
    with urllib.request.urlopen(url_main) as response:
       page_content = response.read()
    print(y)
    html = etree.HTML(page_content)
    pages = html.findall('.//ul[@class="pagesOf"]li')
    try:
        pages_str = pages[1].text
        pages_list = [int(s) for s in pages_str.split() if s.isdigit()]
        pages = pages_list[0]
        for p in range(1,pages+1):
            url = page_navigation_url[:url_year_index+3] + str(years[y]) + page_navigation_url[url_year_index+3:url_p_index+2] + str(p) + page_navigation_url[url_p_index+2+1:]
            with urllib.request.urlopen(url) as response:
               page_content = response.read()
            html = etree.HTML(page_content)
            draws = html.findall('.//p[@class="rs-summary"]')
            text = []
            for draw in draws:
                text_written = draw.text, draw.get('value'), draw.get('querystring')
                full_text = text_written[0]
                text.append(full_text)
            link_draws = html.findall('.//a[@id="Popup"]')
            links = []
            for link_draw in link_draws:
                links_written = link_draw.get("href")
                full_link = "http://eresources.nlb.gov.sg/" + links_written
                links.append(full_link)
            index = []
            for j in range(len(text)):
                if (any(ext in text[j] for ext in list_of_adverb) and any(ext in text[j] for ext in compulsory)):
                    index.append(j)
            index_list = list(set(index))
            links_df = pd.DataFrame(links)[0][index_list]
            full_links_df = pd.concat([full_links_df,links_df])
            print("page " + str(p) + "done")
            if p == 20:
                break
            else:
                continue
    except:
        print("links" + str(years[y]) + "failed")
        continue
    full_links_df.to_csv("links" + str(years[y]) + ".csv")
    print("links" + str(years[y]) + ".csv done")
